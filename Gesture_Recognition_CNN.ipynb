{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "aiHOdyRSsGxC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "if not os.path.exists('sign_mnist'):\n",
        "  # Install Kaggle CLI\n",
        "  !pip install -q kaggle\n",
        "\n",
        "  # Import your kaggle.json API Key\n",
        "  files.upload()\n",
        "\n",
        "  # Moves kaggle.json to the correct location\n",
        "  !mkdir -p ~/.kaggle\n",
        "  !mv kaggle.json ~/.kaggle/\n",
        "  !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "  # Download Sign Language MNIST dataset\n",
        "  !kaggle datasets download -d datamunge/sign-language-mnist\n",
        "\n",
        "  # Extract the dataset\n",
        "  dataset_zip = 'sign-language-mnist.zip'\n",
        "  with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
        "      zip_ref.extractall('sign_mnist')\n",
        "\n",
        "  # Check extracted files\n",
        "  os.listdir('sign_mnist')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "# Function to augment images by rotating them 360 degrees in steps of 10 degrees\n",
        "def augment_with_rotation(images, labels, angles = np.arange(22.5, 360, 22.5) ):\n",
        "    augmented_images = list(images)  # Start with original images\n",
        "    augmented_labels = list(labels)  # Start with original labels\n",
        "    print(angles)\n",
        "\n",
        "\n",
        "    for img, label in zip(images, labels):\n",
        "        # Ensure image is in uint8 format\n",
        "        if img.dtype != np.uint8:\n",
        "            img = (img * 255).astype(np.uint8)\n",
        "\n",
        "        # Ensure image is 2D\n",
        "        if len(img.shape) != 2:\n",
        "            raise ValueError(f\"Expected a 2D image, but got shape: {img.shape}\")\n",
        "\n",
        "        rows, cols = img.shape\n",
        "\n",
        "        for angle in angles:\n",
        "            # Compute rotation matrix\n",
        "            M = cv.getRotationMatrix2D(((cols - 1) / 2.0, (rows - 1) / 2.0), angle, 1)\n",
        "            rotated_img = cv.warpAffine(img, M, (cols, rows))\n",
        "\n",
        "            augmented_images.append(rotated_img)\n",
        "            augmented_labels.append(label)  # Keep the same label for rotated images\n",
        "\n",
        "    return np.array(augmented_images), np.array(augmented_labels)\n"
      ],
      "metadata": {
        "id": "dYD71hm12F8s"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# To make our gesture recognition more robust, we will restrict\n",
        "# our number of classes to hand gestures with very distinct profiles.\n",
        "\n",
        "filtered_labels = [0, 2, 19, 24]\n",
        "\n",
        "\n",
        "# Filter the datasets\n",
        "train_df = pd.read_csv('sign_mnist/sign_mnist_train.csv')\n",
        "train_df = train_df[train_df['label'].isin(filtered_labels)]\n",
        "\n",
        "test_df = pd.read_csv('sign_mnist/sign_mnist_test.csv')\n",
        "test_df = test_df[test_df['label'].isin(filtered_labels)]\n",
        "\n",
        "training_labels = train_df['label'].values\n",
        "training_images = train_df.drop('label', axis=1).values\n",
        "\n",
        "testing_labels = test_df['label'].values\n",
        "testing_images = test_df.drop('label', axis=1).values\n",
        "\n",
        "\n",
        "# Split the data into images and labels\n",
        "# Split training data into training (80%) and validation (20%)\n",
        "training_images, val_images, training_labels, val_labels = train_test_split(\n",
        "    training_images, training_labels, test_size=0.2, random_state=42, stratify=training_labels\n",
        ")\n",
        "\n",
        "# Create a mapping from original labels to new labels\n",
        "# This will convert the labels to 0,1,2,3\n",
        "label_mapping = {label: idx for idx, label in enumerate(filtered_labels)}\n",
        "\n",
        "# Apply the mapping\n",
        "training_labels = np.vectorize(label_mapping.get)(training_labels)\n",
        "testing_labels = np.vectorize(label_mapping.get)(testing_labels)\n",
        "val_labels = np.vectorize(label_mapping.get)(val_labels)\n",
        "\n",
        "# Reshape the images to 28x28\n",
        "training_images = training_images.reshape(-1, 28, 28)\n",
        "testing_images = testing_images.reshape(-1, 28, 28)\n",
        "val_images = val_images.reshape(-1, 28, 28)\n",
        "\n",
        "# Augment the data with rotated versions of every image\n",
        "training_images, training_labels = augment_with_rotation(training_images, training_labels)\n",
        "testing_images, testing_labels = augment_with_rotation(testing_images, testing_labels)\n",
        "val_images, val_labels = augment_with_rotation(val_images, val_labels)\n",
        "\n",
        "\n",
        "# Display an example of each label\n",
        "selected_images = []\n",
        "selected_labels = []\n",
        "\n",
        "for class_label in [0,1,2,3]:\n",
        "    idx = np.where(training_labels == class_label)[0][0]  # First occurrence\n",
        "    selected_images.append(training_images[idx])\n",
        "    selected_labels.append(class_label)\n",
        "\n",
        "plt.figure(figsize=(len(selected_images) * 3, 3))\n",
        "for i, (img, lbl) in enumerate(zip(selected_images, selected_labels)):\n",
        "    plt.subplot(1, len(selected_images), i+1)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(f'Label: {lbl}', fontsize=14)\n",
        "    plt.axis('off')\n",
        "\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "nVF7pM7h1Im2",
        "outputId": "c331a5d3-ac9a-4872-fdf9-e23e6450f0e6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5 270.\n",
            " 292.5 315.  337.5]\n",
            "[ 22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5 270.\n",
            " 292.5 315.  337.5]\n",
            "[ 22.5  45.   67.5  90.  112.5 135.  157.5 180.  202.5 225.  247.5 270.\n",
            " 292.5 315.  337.5]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x300 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "\n",
        "# Normalize images to [0, 1]\n",
        "training_images = training_images.astype(np.float32) / 255.0\n",
        "testing_images = testing_images.astype(np.float32) / 255.0\n",
        "val_images = val_images.astype(np.float32) / 255.0\n",
        "\n",
        "# Ensure images have shape (batch_size, 1, 28, 28)\n",
        "training_images = torch.tensor(training_images).unsqueeze(1)  # Add channel dimension\n",
        "testing_images = torch.tensor(testing_images).unsqueeze(1)  # Add channel dimension\n",
        "val_images = torch.tensor(val_images).unsqueeze(1)  # Add channel dimension\n",
        "\n",
        "\n",
        "training_labels = torch.tensor(training_labels, dtype=torch.long)\n",
        "testing_labels = torch.tensor(testing_labels, dtype=torch.long)\n",
        "val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
        "\n",
        "\n",
        "print(\"Training images shape:\", training_images.shape)  # Should be (num_samples, 1, 28, 28)\n",
        "print(\"Testing images shape:\", testing_images.shape)  # Should be (num_samples, 1, 28, 28)\n",
        "\n"
      ],
      "metadata": {
        "id": "I96LgNpeAyRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582c9541-96eb-4a60-87a0-521ecc9a4bb6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images shape: torch.Size([58544, 1, 28, 28])\n",
            "Testing images shape: torch.Size([19536, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Credit for model goes to https://peyrone.medium.com/comparing-svm-and-cnn-in-recognizing-handwritten-digits-an-overview-5ef06b20194e\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        # First convolution layer: input channels = 1 (grayscale), output = 32\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)  # 28x28 -> 28x28\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # 28x28 -> 14x14\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 14x14 -> 14x14\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)  # 14x14 -> 14x14\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 14x14 -> 7x7\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 64)  # Input size = (64 * 7 * 7)\n",
        "        self.fc2 = nn.Linear(64, 4)  # Output layer with 4 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Conv1 -> ReLU -> MaxPool (28x28 -> 14x14)\n",
        "        x = F.relu(self.conv2(x))  # Conv2 -> ReLU (14x14 -> 14x14)\n",
        "        x = self.pool2(F.relu(self.conv3(x)))  # Conv3 -> ReLU -> MaxPool (14x14 -> 7x7)\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten (7x7 feature map to vector)\n",
        "        x = F.relu(self.fc1(x))  # Fully connected layer\n",
        "        x = self.fc2(x)  # Output layer\n",
        "        return x\n",
        "\n",
        "# Create model\n",
        "model = CNNModel()\n",
        "\n"
      ],
      "metadata": {
        "id": "STdxTog1IYqT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the optimizer (Adam)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the loss function (Sparse Categorical Crossentropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Metrics function for accuracy (can be computed manually during training)\n",
        "def compute_accuracy(outputs, labels):\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    accuracy = correct / labels.size(0)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "h1GlqL2uMagV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Convert the training and testing data to PyTorch tensors\n",
        "train_tensor = TensorDataset(torch.tensor(training_images, dtype=torch.float32), torch.tensor(training_labels, dtype=torch.long))\n",
        "test_tensor = TensorDataset(torch.tensor(testing_images, dtype=torch.float32), torch.tensor(testing_labels, dtype=torch.long))\n",
        "val_tensor = TensorDataset(torch.tensor(val_images, dtype=torch.float32), torch.tensor(val_labels, dtype=torch.long))\n",
        "# Create DataLoader for batching\n",
        "train_loader = DataLoader(train_tensor, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_tensor, batch_size=128, shuffle=False)\n",
        "val_loader = DataLoader(val_tensor, batch_size=128, shuffle=False)\n",
        "\n",
        "# Define optimizer and learning rate scheduler\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "early_stop_patience = 5  # Stop if no improvement for 5 epochs\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "num_epochs = 50  # Max limit\n",
        "\n",
        "# Note to self: change this to stop training at epoch 7,\n",
        "# its overfitting past that -ADB\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
        "\n",
        "    # Reduce LR if validation loss plateaus\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_without_improvement = 0\n",
        "        torch.save(model.state_dict(), \"rotated_gesture_model.pth\")  # Save best model\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= early_stop_patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break  # Stop training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "VlSflAJlMdLO",
        "outputId": "d95e4fb0-d668-45cb-e73f-b1a1bfdc6d2b",
        "collapsed": true
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-51867615eff3>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_tensor = TensorDataset(torch.tensor(training_images, dtype=torch.float32), torch.tensor(training_labels, dtype=torch.long))\n",
            "<ipython-input-39-51867615eff3>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_tensor = TensorDataset(torch.tensor(testing_images, dtype=torch.float32), torch.tensor(testing_labels, dtype=torch.long))\n",
            "<ipython-input-39-51867615eff3>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_tensor = TensorDataset(torch.tensor(val_images, dtype=torch.float32), torch.tensor(val_labels, dtype=torch.long))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss = 0.5069, Val Loss = 0.0790, Val Acc = 0.9747\n",
            "Epoch 2: Train Loss = 0.0449, Val Loss = 0.0212, Val Acc = 0.9930\n",
            "Epoch 3: Train Loss = 0.0148, Val Loss = 0.0116, Val Acc = 0.9960\n",
            "Epoch 4: Train Loss = 0.0072, Val Loss = 0.0042, Val Acc = 0.9991\n",
            "Epoch 5: Train Loss = 0.0043, Val Loss = 0.0046, Val Acc = 0.9988\n",
            "Epoch 6: Train Loss = 0.0041, Val Loss = 0.0259, Val Acc = 0.9910\n",
            "Epoch 7: Train Loss = 0.0052, Val Loss = 0.0010, Val Acc = 0.9999\n",
            "Epoch 8: Train Loss = 0.0003, Val Loss = 0.0003, Val Acc = 1.0000\n",
            "Epoch 9: Train Loss = 0.0001, Val Loss = 0.0002, Val Acc = 1.0000\n",
            "Epoch 10: Train Loss = 0.0000, Val Loss = 0.0001, Val Acc = 1.0000\n",
            "Epoch 11: Train Loss = 0.0000, Val Loss = 0.0001, Val Acc = 1.0000\n",
            "Epoch 12: Train Loss = 0.0000, Val Loss = 0.0001, Val Acc = 1.0000\n",
            "Epoch 13: Train Loss = 0.0000, Val Loss = 0.0001, Val Acc = 1.0000\n",
            "Epoch 14: Train Loss = 0.0000, Val Loss = 0.0001, Val Acc = 1.0000\n",
            "Epoch 15: Train Loss = 0.0000, Val Loss = 0.0000, Val Acc = 1.0000\n",
            "Epoch 16: Train Loss = 0.0000, Val Loss = 0.0000, Val Acc = 1.0000\n",
            "Epoch 17: Train Loss = 0.0000, Val Loss = 0.0000, Val Acc = 1.0000\n",
            "Epoch 18: Train Loss = 0.0000, Val Loss = 0.0000, Val Acc = 1.0000\n",
            "Epoch 19: Train Loss = 0.0000, Val Loss = 0.0000, Val Acc = 1.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-51867615eff3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        test_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "        # Compute accuracy\n",
        "        _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
        "        correct += (predicted == labels).sum().item()  # Count correct predictions\n",
        "        total += labels.size(0)  # Total number of samples\n",
        "\n",
        "# Compute average loss and accuracy\n",
        "test_loss /= len(test_loader)\n",
        "test_acc = correct / total\n",
        "\n",
        "print(f'\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "JkcCUPmZr8BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model.state_dict(), 'rotated_gesture_model.pth')"
      ],
      "metadata": {
        "id": "eUUERlUl7OiR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}